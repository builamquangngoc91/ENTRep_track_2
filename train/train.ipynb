{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install ftfy regex tqdm\n!pip -q install git+https://github.com/openai/CLIP.git\n!pip -q install pytorch-metric-learning","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-14T01:47:49.517261Z","iopub.execute_input":"2025-07-14T01:47:49.517482Z","iopub.status.idle":"2025-07-14T01:49:47.591390Z","shell.execute_reply.started":"2025-07-14T01:47:49.517462Z","shell.execute_reply":"2025-07-14T01:49:47.589799Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m161.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Load dataset\n","metadata":{}},{"cell_type":"code","source":"#train-val-test: 70:15:15\n!gdown 1u9XcSIB4-UMmgSGa_or2B3bBbgiNZe5I\n!unzip -q data_cifar10_ent_split.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T01:50:14.056269Z","iopub.execute_input":"2025-07-14T01:50:14.056727Z","iopub.status.idle":"2025-07-14T01:50:34.722971Z","shell.execute_reply.started":"2025-07-14T01:50:14.056639Z","shell.execute_reply":"2025-07-14T01:50:34.721733Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1u9XcSIB4-UMmgSGa_or2B3bBbgiNZe5I\nFrom (redirected): https://drive.google.com/uc?id=1u9XcSIB4-UMmgSGa_or2B3bBbgiNZe5I&confirm=t&uuid=f807294f-eb81-40db-b0c4-554376971f6f\nTo: /kaggle/working/data_cifar10_ent_split.zip\n100%|██████████████████████████████████████| 1.06G/1.06G [00:10<00:00, 99.3MB/s]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\nimport os\nimport torch\nimport clip\nfrom PIL import Image\nimport torch.nn.functional as F\nimport random\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nimport torch.nn as nn\nfrom pytorch_metric_learning import losses\nfrom random import shuffle\nfrom tqdm import tqdm\nimport torch.nn.init as init\n\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T01:52:45.294484Z","iopub.execute_input":"2025-07-14T01:52:45.295634Z","iopub.status.idle":"2025-07-14T01:52:45.300732Z","shell.execute_reply.started":"2025-07-14T01:52:45.295601Z","shell.execute_reply":"2025-07-14T01:52:45.299763Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Prepare Dataset","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load CLIP model (ViT-B/32)\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\nimg_dir = \"./train\"\nembeddings = {}\n\ndef load_images_recursively(folder):\n    '''\n    Embed all images in the training data and return them in a dictionary\n    Return:\n                embeddings = {\n                    \"img_name\": embedding_tensor,\n                    ...\n                }\n    '''\n    embeddings = {}\n    for root, dirs, files in os.walk(folder):\n        for fn in files:\n            if fn.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n                path = os.path.join(root, fn)\n                try:\n                    image = preprocess(Image.open(path)).unsqueeze(0).to(device)\n                    with torch.no_grad():\n                        emb = model.encode_image(image)\n                        emb = emb / emb.norm(dim=-1, keepdim=True)\n                    rel_path = os.path.relpath(path, img_dir)\n                    embeddings[rel_path] = emb.squeeze(0).cpu()\n                except Exception as e:\n                    print(f\"Failed to load {path}: {e}\")\n    return embeddings\n\nembeddings = load_images_recursively(img_dir)\nprint(f\"Total images loaded: {len(embeddings)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T01:53:15.199800Z","iopub.execute_input":"2025-07-14T01:53:15.200458Z","iopub.status.idle":"2025-07-14T01:56:18.923439Z","shell.execute_reply.started":"2025-07-14T01:53:15.200422Z","shell.execute_reply":"2025-07-14T01:56:18.922426Z"}},"outputs":[{"name":"stderr","text":"100%|███████████████████████████████████████| 338M/338M [00:07<00:00, 49.9MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Total images loaded: 1298\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\ndef create_cls_map(root_dir):\n    \"\"\"\n    Create a mapping from relative image path to its class name.\n\n    Args:\n        root_dir (str): Root directory containing class-named subfolders with images.\n\n    Returns:\n        dict: {\n            \"class_name/image_name.jpg\": \"class_name\",\n            ...\n        }\n    \"\"\"\n    cls_map = {}\n    for cls_name in os.listdir(root_dir):\n        cls_path = os.path.join(root_dir, cls_name)\n        if not os.path.isdir(cls_path):\n            continue\n        for img_name in os.listdir(cls_path):\n            if img_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n                rel_path = f\"{cls_name}/{img_name}\"  # or use os.path.join and os.path.relpath for robustness\n                cls_map[rel_path] = cls_name\n    return cls_map\nroot_dir = img_dir\ncls_map = create_cls_map(root_dir)\nprint(f\"Created cls_map for {len(cls_map)} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T01:56:37.063835Z","iopub.execute_input":"2025-07-14T01:56:37.064623Z","iopub.status.idle":"2025-07-14T01:56:37.073123Z","shell.execute_reply.started":"2025-07-14T01:56:37.064589Z","shell.execute_reply":"2025-07-14T01:56:37.072236Z"}},"outputs":[{"name":"stdout","text":"Created cls_map for 1298 images\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef generate_parent_child(cls_map, embeddings, num_children=3, verbose=False):\n    \"\"\"\n    Generate (parent, top-k children) pairs based on cosine similarity within the same class.\n\n    Returns:\n        - parent_child: dict of {img_name: [child1, child2, ...]}\n        - class_to_imgs: dict of {class_name: [img_name1, img_name2, ...]}\n    \"\"\"\n    class_to_imgs = {}\n    for img_name, cls in cls_map.items():\n        class_to_imgs.setdefault(cls, []).append(img_name)\n\n    parent_child = {}\n    skipped_classes = 0\n\n    for cls, img_list in class_to_imgs.items():\n        valid_imgs = [i for i in img_list if i in embeddings]\n        if len(valid_imgs) < 2:\n            skipped_classes += 1\n            continue  # Không đủ ảnh để tạo parent-child\n\n        for img_name in valid_imgs:\n            anchor_emb = embeddings[img_name].unsqueeze(0)\n\n            others = [i for i in valid_imgs if i != img_name]\n            if not others:\n                continue\n\n            other_embs = torch.stack([embeddings[i] for i in others])\n\n            sim_scores = F.cosine_similarity(anchor_emb, other_embs)\n            topk = min(num_children, len(others))\n            topk_indices = torch.topk(sim_scores, k=topk).indices\n\n            children = [others[i] for i in topk_indices.tolist()]\n            parent_child[img_name] = children\n\n    if verbose:\n        print(f\"✅ Generated parent-child pairs: {len(parent_child)}\")\n        print(f\"⚠️ Skipped classes with <2 valid embeddings: {skipped_classes}\")\n\n    return parent_child, class_to_imgs\n\nparent_child, class_to_imgs = generate_parent_child(cls_map, embeddings, num_children=3)\nprint(f\"Generated parent-child pairs for {len(parent_child)} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T02:45:25.727116Z","iopub.execute_input":"2025-07-14T02:45:25.727529Z","iopub.status.idle":"2025-07-14T02:45:26.341355Z","shell.execute_reply.started":"2025-07-14T02:45:25.727498Z","shell.execute_reply":"2025-07-14T02:45:26.340434Z"}},"outputs":[{"name":"stdout","text":"Generated parent-child pairs for 1298 images\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport random\n\n# Set random seeds for reproducibility\nrandom.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n\n# Define class-wise incompatible (negative) relationships\nnegative_map = {\n    \"nose-right\": [\"nose-left\"],\n    \"nose-left\": [\"nose-right\"],\n    \"ear-right\": [\"ear-left\"],\n    \"ear-left\": [\"ear-right\"],\n    \"vc-open\": [\"vc-closed\", \"throat\"],\n    \"vc-closed\": [\"vc-open\"],\n    \"throat\": [\"vc-open\", \"vc-closed\"],\n}\n\n# --- Function definitions ---\n\ndef get_hard_negative(anchor_name, anchor_emb, anchor_cls, class_to_imgs, embeddings, negative_map):\n    \"\"\"\n    Select the hardest negative image (most similar but from a different class).\n    Priority is given to predefined negative classes.\n    \"\"\"\n    neg_classes = negative_map.get(anchor_cls, None)\n\n    if not neg_classes:\n        neg_cls_candidates = [c for c in class_to_imgs if c != anchor_cls]\n        if not neg_cls_candidates:\n            return None\n        neg_cls = random.choice(neg_cls_candidates)\n        neg_imgs = class_to_imgs[neg_cls]\n    else:\n        neg_imgs = []\n        for neg_cls in neg_classes:\n            neg_imgs.extend(class_to_imgs.get(neg_cls, []))\n        if not neg_imgs:\n            return None\n\n    anchor_emb = anchor_emb.unsqueeze(0)\n    max_sim = -1\n    hard_neg = None\n    for neg_img in neg_imgs:\n        if neg_img not in embeddings:\n            continue\n        neg_emb = embeddings[neg_img].unsqueeze(0)\n        sim = F.cosine_similarity(anchor_emb, neg_emb).item()\n        if sim > max_sim:\n            max_sim = sim\n            hard_neg = neg_img\n\n    return hard_neg\n\n\ndef filter_parent_child(parent_child_full, embeddings_subset):\n    \"\"\"\n    Filter parent-child mapping để giữ các cặp có mặt trong embeddings hiện tại.\n    \"\"\"\n    parent_child_filtered = {}\n    valid_imgs = set(embeddings_subset.keys())\n    for parent, children in parent_child_full.items():\n        if parent in valid_imgs:\n            filtered = [c for c in children if c in valid_imgs]\n            if filtered:\n                parent_child_filtered[parent] = filtered\n    return parent_child_filtered\n\n\ndef create_triplets(embeddings, cls_map, parent_child, class_to_imgs):\n    \"\"\"\n    Tạo các bộ 3 (anchor, positive, negative) từ tập embeddings hiện tại.\n    \"\"\"\n    triplets = []\n    for anchor_name, anchor_emb in embeddings.items():\n        if anchor_name not in parent_child or len(parent_child[anchor_name]) == 0:\n            continue\n        anchor_cls = cls_map[anchor_name]\n        pos_name = parent_child[anchor_name][0]\n        neg_name = get_hard_negative(anchor_name, anchor_emb, anchor_cls, class_to_imgs, embeddings, negative_map)\n\n        if neg_name is not None:\n            triplets.append((\n                anchor_emb,\n                embeddings[pos_name],\n                embeddings[neg_name],\n                anchor_name,\n                neg_name\n            ))\n\n    print(f\"Generated {len(triplets)} triplets.\")\n    return triplets\n\n\ndef get_triplet_batch(triplets, batch_size=32):\n    if len(triplets) < batch_size:\n        return None\n    batch = random.sample(triplets, batch_size)\n    anchor = torch.stack([t[0] for t in batch])\n    positive = torch.stack([t[1] for t in batch])\n    negative = torch.stack([t[2] for t in batch])\n    return anchor, positive, negative\n\n\nparent_child = filter_parent_child(parent_child, embeddings)\ntriplets = create_triplets(embeddings, cls_map, parent_child, class_to_imgs)\nbatch = get_triplet_batch(triplets, batch_size=32)\nif batch:\n    anchor, positive, negative = batch\n    print(anchor.shape, positive.shape, negative.shape)\nelse:\n    print(\"Không đủ triplet để tạo batch.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T02:00:40.317516Z","iopub.execute_input":"2025-07-14T02:00:40.317867Z","iopub.status.idle":"2025-07-14T02:00:50.570162Z","shell.execute_reply.started":"2025-07-14T02:00:40.317841Z","shell.execute_reply":"2025-07-14T02:00:50.569332Z"}},"outputs":[{"name":"stdout","text":"Generated 1298 triplets.\ntorch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 512])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"embed_dim = 512\nclass GaussianFourierProjection(nn.Module):\n    def __init__(self, embed_dim, scale=10.0):\n        super().__init__()\n        # Fixed random weights for projecting scalar t to higher frequency space\n        self.W = nn.Parameter(torch.randn(1, embed_dim // 2) * scale, requires_grad=False)\n\n    def forward(self, t):\n        # Ensure t has shape [B, 1]\n        if t.ndim == 1:\n            t = t.unsqueeze(-1)\n        proj = t * self.W  # Shape: [B, D/2]\n        # Return sinusoidal and cosinusoidal projection: [sin(tW), cos(tW)] → Shape: [B, D]\n        return torch.cat([torch.sin(proj), torch.cos(proj)], dim=-1)\n\nclass VectorField(nn.Module):\n    def __init__(self, dim, t_dim=32, hidden_dim=256, n_heads=4, dropout_prob=0.1):\n        super().__init__()\n        self.x_norm = nn.LayerNorm(dim)  # Normalize input embeddings\n        self.time_encoder = GaussianFourierProjection(t_dim)  # Time embedding module\n        self.dropout = nn.Dropout(dropout_prob)\n\n        # Create multiple independent heads (like a lightweight transformer block)\n        self.heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(dim + t_dim, hidden_dim),     # Project input + time\n                nn.LayerNorm(hidden_dim),\n                nn.SiLU(),                               # Activation: SiLU \n                nn.Dropout(dropout_prob),\n                nn.Linear(hidden_dim, dim)              # Back to original embedding dimension\n            ) for _ in range(n_heads)\n        ])\n\n        self.res_weight = nn.Parameter(torch.tensor(1.0))  # Learnable residual scaling\n        self.out_norm = nn.LayerNorm(dim)  # Final normalization (not applied directly here)\n        self.initialize_weights()\n\n    \n    def initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                # Use Kaiming initialization (good for ReLU/SiLU)\n                init.kaiming_uniform_(m.weight, nonlinearity='relu')\n                if m.bias is not None:\n                    init.zeros_(m.bias)\n\n\n    def forward(self, x, t):\n        # Handle scalar or 1D tensor time input → ensure shape [B, 1]\n        if not isinstance(t, torch.Tensor):\n            t = torch.full((x.shape[0], 1), t, device=x.device)\n        elif t.ndim == 0:\n            t = t.expand(x.shape[0], 1)\n        elif t.ndim == 1:\n            t = t.unsqueeze(-1)\n\n        x_normed = self.x_norm(x)                     # Normalize input\n        t_encoded = self.time_encoder(t.to(x.device)) # Encode time t\n        inp = torch.cat([x_normed, t_encoded], dim=-1)  # Concatenate along feature dim\n\n        # Pass through each head and average their outputs\n        head_outs = [head(inp) for head in self.heads]\n        out = torch.mean(torch.stack(head_outs), dim=0)\n\n        # Add residual connection scaled by learnable weight\n        return out + self.res_weight * x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T02:05:29.673566Z","iopub.execute_input":"2025-07-14T02:05:29.673983Z","iopub.status.idle":"2025-07-14T02:05:29.686757Z","shell.execute_reply.started":"2025-07-14T02:05:29.673955Z","shell.execute_reply":"2025-07-14T02:05:29.685753Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def euler_integration(x0, vf, steps=10):\n    \"\"\"\n    Args:\n        x0: initial embeddings [B, D]\n        vf: vector field model (takes in x and t, returns dx/dt)\n        steps: number of integration steps\n    Returns:\n        Transformed embeddings x(T)\n    \"\"\"\n    dt = 1.0 / steps\n    x = x0\n    for i in range(steps):\n        t = i * dt\n        k1 = vf(x, t)\n        k2 = vf(x + 0.5 * dt * k1, t + 0.5 * dt)\n        k3 = vf(x + 0.5 * dt * k2, t + 0.5 * dt)\n        k4 = vf(x + dt * k3, t + dt)\n        x = x + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n    return x\n\n    #return correct / total if total > 0 else 0.0\ndef compute_triplet_loss(model, triplets, loss_func, cls_map, steps=10):\n    \"\"\"\n       Compute loss using MultiSimilarityLoss\n       It takes transformed embeddings + integer labels\n   \n    \"\"\"\n    model.eval()\n    total_loss = 0.0\n    num_batches = 0\n\n    with torch.no_grad():\n        for i in range(0, len(triplets), 64):\n            batch = triplets[i : i + 64]\n            if len(batch) == 0:\n                continue\n\n            batch_embeddings, batch_labels = get_embeddings_labels_from_triplets(batch, cls_map, class2idx)\n            if batch_embeddings is None or len(batch_embeddings) == 0:\n                continue\n\n            batch_embeddings = batch_embeddings.to(device).float()\n            batch_labels = batch_labels.to(device).long()\n\n            pred_embeddings = euler_integration(batch_embeddings, model, steps=steps)\n            loss = loss_func(pred_embeddings, batch_labels)\n\n            total_loss += loss.item()\n            num_batches += 1\n\n    return total_loss / max(1, num_batches)\n\ndef get_embeddings_labels_from_triplets(triplets_batch, cls_map, class2idx):\n    '''\n        Convert list of triplets into tensors for model input.\n    '''\n    embeddings = []\n    labels = []\n\n    for anchor_emb, positive_emb, negative_emb, anchor_name, negative_name in triplets_batch:\n        anchor_cls = cls_map[anchor_name]\n        negative_cls = cls_map[negative_name]\n\n        embeddings.extend([anchor_emb, positive_emb, negative_emb])\n        labels.extend([\n            class2idx[anchor_cls],\n            class2idx[anchor_cls],\n            class2idx[negative_cls]\n        ])\n\n    embeddings_tensor = torch.stack(embeddings)\n    labels_tensor = torch.tensor(labels)\n    return embeddings_tensor, labels_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T02:52:57.198572Z","iopub.execute_input":"2025-07-14T02:52:57.199367Z","iopub.status.idle":"2025-07-14T02:52:57.209918Z","shell.execute_reply.started":"2025-07-14T02:52:57.199336Z","shell.execute_reply":"2025-07-14T02:52:57.208946Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"root_dir_val = \"./val\"\n\ndef standardize_keys(d):\n    return {\n        os.path.join(os.path.basename(os.path.dirname(k)), os.path.basename(k)): v\n        for k, v in d.items()\n    }\n\n# 1. Load embeddings\nembeddings_val = load_images_recursively(root_dir_val)\nprint(f\" Total images loaded: {len(embeddings_val)}\")\n\n# 2. Load cls_map\ncls_map_val = create_cls_map(root_dir_val)\nprint(f\" Created cls_map for {len(cls_map_val)} images\")\n\n# 3.  Standardize keys\nembeddings_val = standardize_keys(embeddings_val)\ncls_map_val = standardize_keys(cls_map_val)\n\n# 4. Generate parent-child\nparent_child_val, class_to_imgs_val = generate_parent_child(cls_map_val, embeddings_val, num_children=3, verbose=True)\n\n# 5. Filter parent-child\nparent_child_val = filter_parent_child(parent_child_val, embeddings_val)\n\n# 6. Create triplets\ntriplets_val = create_triplets(embeddings_val, cls_map_val, parent_child_val, class_to_imgs_val)\nprint(f\" Total triplets generated: {len(triplets_val)}\")\n\n# 7. Get batch\nbatch_val = get_triplet_batch(triplets_val, batch_size=32)\nif batch_val:\n    anchor_val, positive_val, negative_val = batch_val\n    print(\" Batch shapes:\", anchor_val.shape, positive_val.shape, negative_val.shape)\nelse:\n    print(\" Không đủ triplet để tạo batch.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T02:46:23.232842Z","iopub.execute_input":"2025-07-14T02:46:23.233183Z","iopub.status.idle":"2025-07-14T02:47:01.488551Z","shell.execute_reply.started":"2025-07-14T02:46:23.233159Z","shell.execute_reply":"2025-07-14T02:47:01.487523Z"}},"outputs":[{"name":"stdout","text":" Total images loaded: 275\n Created cls_map for 275 images\n✅ Generated parent-child pairs: 275\n⚠️ Skipped classes with <2 valid embeddings: 0\nGenerated 275 triplets.\n Total triplets generated: 275\n Batch shapes: torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32, 512])\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"sample_cls_key = list(cls_map_val.keys())[0]\nsample_emb_key = list(embeddings_val.keys())[0]\nprint(\"🔍 Sample key từ cls_map:\", sample_cls_key)\nprint(\"🔍 Sample key từ embeddings:\", sample_emb_key)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T02:47:04.625085Z","iopub.execute_input":"2025-07-14T02:47:04.625373Z","iopub.status.idle":"2025-07-14T02:47:04.631480Z","shell.execute_reply.started":"2025-07-14T02:47:04.625352Z","shell.execute_reply":"2025-07-14T02:47:04.630143Z"}},"outputs":[{"name":"stdout","text":"🔍 Sample key từ cls_map: vc-open/23146955_230624145953203831_954_image04.png\n🔍 Sample key từ embeddings: vc-open/23146955_230624145953203831_954_image04.png\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"@torch.no_grad()\ndef compute_recall_at_1_embedded(embeddings, cls_map, type_map, vf, device=\"cuda\", steps=10):\n    \"\"\"\n    embeddings: dict tên ảnh → vector trước khi qua VF\n    cls_map: dict tên ảnh → class (string)\n    type_map: dict class → list các class cùng type\n    vf: mô hình vector field (đã học)\n    \"\"\"\n    keys = list(embeddings.keys())\n    embedded_vectors = {}\n    \n    # Apply VF để lấy vector sau khi flow\n    for k in keys:\n        emb = embeddings[k].unsqueeze(0).to(device).float()\n        emb = euler_integration(emb, vf, steps=steps)\n        embedded_vectors[k] = emb.squeeze(0).cpu()  # chuyển lại về CPU để so cosine\n\n    correct = 0\n    total = 0\n\n    for anchor_name in keys:\n        anchor_emb = embedded_vectors[anchor_name].unsqueeze(0)\n        anchor_cls = cls_map[anchor_name]\n        anchor_type = set(type_map.get(anchor_cls, [])) | {anchor_cls}\n\n        max_sim = -float(\"inf\")\n        top1_name = None\n\n        for other_name in keys:\n            if other_name == anchor_name:\n                continue\n            other_emb = embedded_vectors[other_name].unsqueeze(0)\n            sim = F.cosine_similarity(anchor_emb, other_emb).item()\n            if sim > max_sim:\n                max_sim = sim\n                top1_name = other_name\n\n        if top1_name:\n            pred_cls = cls_map[top1_name]\n            if pred_cls in anchor_type:\n                correct += 1\n        total += 1\n\n    recall = correct / total if total > 0 else 0.0\n    return recall\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T02:47:11.209923Z","iopub.execute_input":"2025-07-14T02:47:11.210246Z","iopub.status.idle":"2025-07-14T02:47:11.219590Z","shell.execute_reply.started":"2025-07-14T02:47:11.210221Z","shell.execute_reply":"2025-07-14T02:47:11.218563Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"#Define label mapping\nclass2idx = {'ear-left': 3,\n 'ear-right': 2,\n 'nose-left': 1,\n 'nose-right': 0,\n 'throat': 6,\n 'vc-closed': 4,\n 'vc-open': 5}\n\n# Khởi tạo model, optimizer, loss\nvf = VectorField(embed_dim).to(device).float()\noptimizer = torch.optim.AdamW(vf.parameters(), lr=1e-4)\nloss_func = losses.MultiSimilarityLoss()\n\n# Hyperparameters cho scheduler và early stopping\nwarmup_epochs = 20\nearly_stop_patience = 40\nscheduler_patience = 15\nscheduler_factor = 0.8\ndelta = 5e-4\n\n# LR Scheduler\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience)\n\n# Biến theo dõi\nbest_val_loss = float('inf')\nbest_train_loss = float('inf')\nepochs_no_improve = 0\n\n# Huấn luyện\nepochs = 200\nbatch_size = 32\n\nfor epoch in tqdm(range(epochs)):\n    vf.train()\n    epoch_loss = 0.0\n    num_batches = 0\n\n    random.seed(42)\n    random.shuffle(triplets)\n\n    for i in range(0, len(triplets), batch_size):\n        batch = triplets[i: i + batch_size]\n        if not batch:\n            continue\n\n        optimizer.zero_grad()\n        batch_embeddings, batch_labels = get_embeddings_labels_from_triplets(batch, cls_map, class2idx)\n\n        if batch_embeddings is None or len(batch_embeddings) == 0:\n            continue\n\n        batch_embeddings = batch_embeddings.to(device).float()\n        batch_labels = batch_labels.to(device).long()\n        pred_embeddings = euler_integration(batch_embeddings, vf, steps=10)\n\n        loss = loss_func(pred_embeddings, batch_labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(vf.parameters(), max_norm=5.0)\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        num_batches += 1\n\n    # --- Evaluation ---\n    vf.eval()\n    val_loss = compute_triplet_loss(vf, triplets_val, loss_func,cls_map_val, steps=10)\n    avg_train_loss = epoch_loss / max(1, num_batches)\n\n\n    print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f}\")\n\n    # --- LR Warmup & Scheduler ---\n    old_lr = optimizer.param_groups[0]['lr']\n    if epoch >= warmup_epochs:\n        scheduler.step(val_loss)\n    else:\n        scheduler.step(float('inf'))\n\n    new_lr = optimizer.param_groups[0]['lr']\n    if new_lr < old_lr:\n        print(f\"⚠️ LR reduced at epoch {epoch + 1} → {new_lr:.1e}\")\n\n    # --- Early Stopping ---\n    if (val_loss < best_val_loss - delta) or (avg_train_loss < best_train_loss - delta):\n        best_val_loss = min(val_loss, best_val_loss)\n        best_train_loss = min(avg_train_loss, best_train_loss)\n        epochs_no_improve = 0\n        torch.save(vf.state_dict(), \"best_vf.pt\")\n    else:\n        epochs_no_improve += 1\n\n    if epochs_no_improve >= early_stop_patience:\n        print(f\"⛔ Early stopping at epoch {epoch + 1}\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T05:41:47.959216Z","iopub.execute_input":"2025-07-14T05:41:47.959523Z","iopub.status.idle":"2025-07-14T06:43:31.023060Z","shell.execute_reply.started":"2025-07-14T05:41:47.959498Z","shell.execute_reply":"2025-07-14T06:43:31.019553Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 1/200 [00:18<59:51, 18.05s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] Train Loss: 1.4995\n","output_type":"stream"},{"name":"stderr","text":"  1%|          | 2/200 [00:36<1:00:39, 18.38s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] Train Loss: 1.5001\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 3/200 [00:54<1:00:00, 18.28s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] Train Loss: 1.4906\n","output_type":"stream"},{"name":"stderr","text":"  2%|▏         | 4/200 [01:13<1:00:05, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] Train Loss: 1.4958\n","output_type":"stream"},{"name":"stderr","text":"  2%|▎         | 5/200 [01:32<1:00:10, 18.51s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] Train Loss: 1.4962\n","output_type":"stream"},{"name":"stderr","text":"  3%|▎         | 6/200 [01:50<59:21, 18.36s/it]  ","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] Train Loss: 1.4896\n","output_type":"stream"},{"name":"stderr","text":"  4%|▎         | 7/200 [02:08<59:16, 18.43s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] Train Loss: 1.4866\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 8/200 [02:26<58:22, 18.24s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] Train Loss: 1.4780\n","output_type":"stream"},{"name":"stderr","text":"  4%|▍         | 9/200 [02:44<58:07, 18.26s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] Train Loss: 1.4845\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 10/200 [03:02<57:34, 18.18s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] Train Loss: 1.4754\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 11/200 [03:21<57:47, 18.35s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 11] Train Loss: 1.4837\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 12/200 [03:40<57:40, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 12] Train Loss: 1.4841\n","output_type":"stream"},{"name":"stderr","text":"  6%|▋         | 13/200 [03:57<56:48, 18.23s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 13] Train Loss: 1.4784\n","output_type":"stream"},{"name":"stderr","text":"  7%|▋         | 14/200 [04:16<56:49, 18.33s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 14] Train Loss: 1.4861\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 15/200 [04:34<55:59, 18.16s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 15] Train Loss: 1.4482\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 16/200 [04:53<56:32, 18.44s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 16] Train Loss: 1.4393\n⚠️ LR reduced at epoch 16 → 8.0e-05\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 17/200 [05:11<55:56, 18.34s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 17] Train Loss: 1.4252\n","output_type":"stream"},{"name":"stderr","text":"  9%|▉         | 18/200 [05:30<55:58, 18.46s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 18] Train Loss: 1.4029\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 19/200 [05:49<56:05, 18.59s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 19] Train Loss: 1.3962\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 20/200 [06:07<55:10, 18.39s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 20] Train Loss: 1.4029\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 21/200 [06:25<55:00, 18.44s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 21] Train Loss: 1.3982\n","output_type":"stream"},{"name":"stderr","text":" 11%|█         | 22/200 [06:43<54:22, 18.33s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 22] Train Loss: 1.3827\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 23/200 [07:02<54:35, 18.50s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 23] Train Loss: 1.3538\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 24/200 [07:20<54:06, 18.44s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 24] Train Loss: 1.3554\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▎        | 25/200 [07:39<54:10, 18.57s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 25] Train Loss: 1.3582\n","output_type":"stream"},{"name":"stderr","text":" 13%|█▎        | 26/200 [07:58<53:57, 18.61s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 26] Train Loss: 1.3580\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▎        | 27/200 [08:16<53:18, 18.49s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 27] Train Loss: 1.3527\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 28/200 [08:35<53:17, 18.59s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 28] Train Loss: 1.3382\n","output_type":"stream"},{"name":"stderr","text":" 14%|█▍        | 29/200 [08:54<52:58, 18.59s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 29] Train Loss: 1.3273\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 30/200 [09:12<52:48, 18.64s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 30] Train Loss: 1.3350\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 31/200 [09:31<52:29, 18.64s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 31] Train Loss: 1.3169\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▌        | 32/200 [09:49<51:45, 18.48s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 32] Train Loss: 1.3260\n","output_type":"stream"},{"name":"stderr","text":" 16%|█▋        | 33/200 [10:08<51:29, 18.50s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 33] Train Loss: 1.3011\n","output_type":"stream"},{"name":"stderr","text":" 17%|█▋        | 34/200 [10:25<50:36, 18.29s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 34] Train Loss: 1.2942\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 35/200 [10:44<50:36, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 35] Train Loss: 1.2955\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 36/200 [11:02<49:53, 18.25s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 36] Train Loss: 1.3073\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 37/200 [11:21<50:04, 18.43s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 37] Train Loss: 1.2880\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 38/200 [11:40<50:11, 18.59s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 38] Train Loss: 1.2827\n⚠️ LR reduced at epoch 38 → 6.4e-05\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 39/200 [11:59<49:59, 18.63s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 39] Train Loss: 1.2842\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 40/200 [12:17<49:50, 18.69s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 40] Train Loss: 1.2727\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 41/200 [12:35<48:57, 18.47s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 41] Train Loss: 1.2559\n","output_type":"stream"},{"name":"stderr","text":" 21%|██        | 42/200 [12:54<48:43, 18.50s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 42] Train Loss: 1.2651\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 43/200 [13:12<48:09, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 43] Train Loss: 1.2594\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▏       | 44/200 [13:31<48:02, 18.48s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 44] Train Loss: 1.2644\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▎       | 45/200 [13:51<48:52, 18.92s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 45] Train Loss: 1.2563\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 46/200 [14:09<48:01, 18.71s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 46] Train Loss: 1.2541\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▎       | 47/200 [14:28<47:48, 18.75s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 47] Train Loss: 1.2524\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 48/200 [14:46<46:51, 18.50s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 48] Train Loss: 1.2571\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 49/200 [15:04<46:41, 18.55s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 49] Train Loss: 1.2682\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 50/200 [15:22<46:03, 18.42s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 50] Train Loss: 1.2708\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 51/200 [15:41<45:57, 18.51s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 51] Train Loss: 1.2693\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▌       | 52/200 [16:00<46:15, 18.75s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 52] Train Loss: 1.2583\n","output_type":"stream"},{"name":"stderr","text":" 26%|██▋       | 53/200 [16:19<45:38, 18.63s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 53] Train Loss: 1.2468\n","output_type":"stream"},{"name":"stderr","text":" 27%|██▋       | 54/200 [16:39<46:07, 18.95s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 54] Train Loss: 1.2541\n⚠️ LR reduced at epoch 54 → 5.1e-05\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 55/200 [16:58<46:11, 19.11s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 55] Train Loss: 1.2456\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 56/200 [17:17<46:08, 19.23s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 56] Train Loss: 1.2474\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 57/200 [17:37<45:46, 19.21s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 57] Train Loss: 1.2400\n","output_type":"stream"},{"name":"stderr","text":" 29%|██▉       | 58/200 [17:55<44:52, 18.96s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 58] Train Loss: 1.2433\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 59/200 [18:14<44:29, 18.93s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 59] Train Loss: 1.2512\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 60/200 [18:32<43:43, 18.74s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 60] Train Loss: 1.2375\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 61/200 [18:51<43:40, 18.85s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 61] Train Loss: 1.2366\n","output_type":"stream"},{"name":"stderr","text":" 31%|███       | 62/200 [19:10<43:26, 18.89s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 62] Train Loss: 1.2457\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 63/200 [19:29<42:48, 18.75s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 63] Train Loss: 1.2423\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▏      | 64/200 [19:48<42:45, 18.86s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 64] Train Loss: 1.2518\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▎      | 65/200 [20:06<41:59, 18.66s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 65] Train Loss: 1.2484\n","output_type":"stream"},{"name":"stderr","text":" 33%|███▎      | 66/200 [20:25<41:47, 18.72s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 66] Train Loss: 1.2462\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▎      | 67/200 [20:43<41:12, 18.59s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 67] Train Loss: 1.2313\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 68/200 [21:02<41:01, 18.65s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 68] Train Loss: 1.2416\n","output_type":"stream"},{"name":"stderr","text":" 34%|███▍      | 69/200 [21:21<40:50, 18.71s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 69] Train Loss: 1.2325\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 70/200 [21:39<40:14, 18.57s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 70] Train Loss: 1.2342\n⚠️ LR reduced at epoch 70 → 4.1e-05\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 71/200 [21:58<39:58, 18.59s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 71] Train Loss: 1.2295\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 72/200 [22:16<39:20, 18.44s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 72] Train Loss: 1.2397\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▋      | 73/200 [22:34<39:08, 18.49s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 73] Train Loss: 1.2355\n","output_type":"stream"},{"name":"stderr","text":" 37%|███▋      | 74/200 [22:53<38:57, 18.55s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 74] Train Loss: 1.2319\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 75/200 [23:11<38:19, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 75] Train Loss: 1.2332\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 76/200 [23:30<38:15, 18.51s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 76] Train Loss: 1.2267\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 77/200 [23:48<37:26, 18.26s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 77] Train Loss: 1.2291\n","output_type":"stream"},{"name":"stderr","text":" 39%|███▉      | 78/200 [24:06<37:15, 18.32s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 78] Train Loss: 1.2291\n","output_type":"stream"},{"name":"stderr","text":" 40%|███▉      | 79/200 [24:24<36:45, 18.23s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 79] Train Loss: 1.2259\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 80/200 [24:43<36:59, 18.49s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 80] Train Loss: 1.2272\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 81/200 [25:02<36:50, 18.57s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 81] Train Loss: 1.2248\n","output_type":"stream"},{"name":"stderr","text":" 41%|████      | 82/200 [25:20<36:23, 18.51s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 82] Train Loss: 1.2271\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 83/200 [25:39<36:21, 18.65s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 83] Train Loss: 1.2408\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 84/200 [25:58<36:06, 18.68s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 84] Train Loss: 1.2214\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▎     | 85/200 [26:17<35:53, 18.73s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 85] Train Loss: 1.2275\n","output_type":"stream"},{"name":"stderr","text":" 43%|████▎     | 86/200 [26:35<35:11, 18.52s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 86] Train Loss: 1.2287\n⚠️ LR reduced at epoch 86 → 3.3e-05\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▎     | 87/200 [26:54<34:59, 18.58s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 87] Train Loss: 1.2225\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▍     | 88/200 [27:12<34:45, 18.62s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 88] Train Loss: 1.2208\n","output_type":"stream"},{"name":"stderr","text":" 44%|████▍     | 89/200 [27:30<34:06, 18.43s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 89] Train Loss: 1.2167\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 90/200 [27:49<33:53, 18.48s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 90] Train Loss: 1.2160\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 91/200 [28:07<33:16, 18.31s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 91] Train Loss: 1.2160\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▌     | 92/200 [28:25<33:03, 18.36s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 92] Train Loss: 1.2204\n","output_type":"stream"},{"name":"stderr","text":" 46%|████▋     | 93/200 [28:43<32:36, 18.28s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 93] Train Loss: 1.2105\n","output_type":"stream"},{"name":"stderr","text":" 47%|████▋     | 94/200 [29:02<32:36, 18.45s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 94] Train Loss: 1.2228\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 95/200 [29:21<32:28, 18.56s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 95] Train Loss: 1.2046\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 96/200 [29:39<31:54, 18.41s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 96] Train Loss: 1.1907\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 97/200 [29:58<31:56, 18.61s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 97] Train Loss: 1.2015\n","output_type":"stream"},{"name":"stderr","text":" 49%|████▉     | 98/200 [30:17<31:33, 18.56s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 98] Train Loss: 1.1760\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 99/200 [30:36<31:28, 18.70s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 99] Train Loss: 1.1720\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 100/200 [30:54<31:03, 18.64s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 100] Train Loss: 1.1655\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 101/200 [31:13<30:59, 18.79s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 101] Train Loss: 1.1616\n","output_type":"stream"},{"name":"stderr","text":" 51%|█████     | 102/200 [31:32<30:29, 18.67s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 102] Train Loss: 1.1333\n⚠️ LR reduced at epoch 102 → 2.6e-05\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 103/200 [31:50<29:46, 18.42s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 103] Train Loss: 1.1207\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▏    | 104/200 [32:08<29:32, 18.47s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 104] Train Loss: 1.1119\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▎    | 105/200 [32:26<29:01, 18.33s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 105] Train Loss: 1.0996\n","output_type":"stream"},{"name":"stderr","text":" 53%|█████▎    | 106/200 [32:44<28:43, 18.34s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 106] Train Loss: 1.1197\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▎    | 107/200 [33:03<28:20, 18.29s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 107] Train Loss: 1.0909\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 108/200 [33:21<28:01, 18.27s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 108] Train Loss: 1.0799\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▍    | 109/200 [33:39<27:45, 18.30s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 109] Train Loss: 1.0716\n","output_type":"stream"},{"name":"stderr","text":" 55%|█████▌    | 110/200 [33:57<27:06, 18.07s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 110] Train Loss: 1.0754\n","output_type":"stream"},{"name":"stderr","text":" 56%|█████▌    | 111/200 [34:15<26:53, 18.12s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 111] Train Loss: 1.1003\n","output_type":"stream"},{"name":"stderr","text":" 56%|█████▌    | 112/200 [34:33<26:28, 18.06s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 112] Train Loss: 1.0656\n","output_type":"stream"},{"name":"stderr","text":" 56%|█████▋    | 113/200 [34:52<26:25, 18.23s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 113] Train Loss: 1.0732\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▋    | 114/200 [35:10<26:03, 18.18s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 114] Train Loss: 1.0474\n","output_type":"stream"},{"name":"stderr","text":" 57%|█████▊    | 115/200 [35:28<26:02, 18.38s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 115] Train Loss: 1.0467\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 116/200 [35:47<25:44, 18.39s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 116] Train Loss: 1.0648\n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 117/200 [36:05<25:14, 18.25s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 117] Train Loss: 1.0544\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 118/200 [36:24<25:10, 18.42s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 118] Train Loss: 1.0364\n⚠️ LR reduced at epoch 118 → 2.1e-05\n","output_type":"stream"},{"name":"stderr","text":" 60%|█████▉    | 119/200 [36:42<24:50, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 119] Train Loss: 1.0395\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 120/200 [37:01<24:55, 18.69s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 120] Train Loss: 1.0377\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 121/200 [37:19<24:21, 18.50s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 121] Train Loss: 1.0376\n","output_type":"stream"},{"name":"stderr","text":" 61%|██████    | 122/200 [37:37<23:49, 18.33s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 122] Train Loss: 1.0283\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 123/200 [37:56<23:45, 18.51s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 123] Train Loss: 1.0113\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▏   | 124/200 [38:14<23:15, 18.36s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 124] Train Loss: 1.0325\n","output_type":"stream"},{"name":"stderr","text":" 62%|██████▎   | 125/200 [38:33<23:00, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 125] Train Loss: 1.0214\n","output_type":"stream"},{"name":"stderr","text":" 63%|██████▎   | 126/200 [38:51<22:34, 18.31s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 126] Train Loss: 1.0167\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▎   | 127/200 [39:10<22:29, 18.48s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 127] Train Loss: 1.0213\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▍   | 128/200 [39:29<22:22, 18.64s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 128] Train Loss: 1.0169\n","output_type":"stream"},{"name":"stderr","text":" 64%|██████▍   | 129/200 [39:47<22:00, 18.60s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 129] Train Loss: 1.0087\n","output_type":"stream"},{"name":"stderr","text":" 65%|██████▌   | 130/200 [40:06<21:54, 18.78s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 130] Train Loss: 1.0319\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▌   | 131/200 [40:24<21:19, 18.55s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 131] Train Loss: 1.0080\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▌   | 132/200 [40:43<21:00, 18.54s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 132] Train Loss: 1.0384\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▋   | 133/200 [41:01<20:30, 18.37s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 133] Train Loss: 1.0154\n","output_type":"stream"},{"name":"stderr","text":" 67%|██████▋   | 134/200 [41:20<20:21, 18.51s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 134] Train Loss: 1.0159\n⚠️ LR reduced at epoch 134 → 1.7e-05\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 135/200 [41:39<20:07, 18.58s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 135] Train Loss: 1.0019\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 136/200 [41:58<19:56, 18.69s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 136] Train Loss: 1.0076\n","output_type":"stream"},{"name":"stderr","text":" 68%|██████▊   | 137/200 [42:16<19:42, 18.76s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 137] Train Loss: 1.0004\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 138/200 [42:35<19:11, 18.58s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 138] Train Loss: 1.0067\n","output_type":"stream"},{"name":"stderr","text":" 70%|██████▉   | 139/200 [42:53<18:55, 18.61s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 139] Train Loss: 1.0184\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 140/200 [43:11<18:27, 18.46s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 140] Train Loss: 1.0073\n","output_type":"stream"},{"name":"stderr","text":" 70%|███████   | 141/200 [43:30<18:13, 18.53s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 141] Train Loss: 0.9919\n","output_type":"stream"},{"name":"stderr","text":" 71%|███████   | 142/200 [43:49<17:54, 18.53s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 142] Train Loss: 0.9962\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 143/200 [44:07<17:30, 18.43s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 143] Train Loss: 0.9823\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 144/200 [44:26<17:18, 18.54s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 144] Train Loss: 0.9969\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▎  | 145/200 [44:44<16:55, 18.46s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 145] Train Loss: 1.0032\n","output_type":"stream"},{"name":"stderr","text":" 73%|███████▎  | 146/200 [45:02<16:38, 18.49s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 146] Train Loss: 1.0165\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▎  | 147/200 [45:21<16:14, 18.39s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 147] Train Loss: 1.0121\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 148/200 [45:40<16:04, 18.55s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 148] Train Loss: 0.9892\n","output_type":"stream"},{"name":"stderr","text":" 74%|███████▍  | 149/200 [45:58<15:45, 18.54s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 149] Train Loss: 0.9980\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 150/200 [46:16<15:16, 18.33s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 150] Train Loss: 1.0080\n⚠️ LR reduced at epoch 150 → 1.3e-05\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 151/200 [46:34<14:59, 18.36s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 151] Train Loss: 0.9854\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▌  | 152/200 [46:52<14:35, 18.24s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 152] Train Loss: 0.9849\n","output_type":"stream"},{"name":"stderr","text":" 76%|███████▋  | 153/200 [47:11<14:21, 18.34s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 153] Train Loss: 1.0054\n","output_type":"stream"},{"name":"stderr","text":" 77%|███████▋  | 154/200 [47:29<14:00, 18.27s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 154] Train Loss: 0.9889\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 155/200 [47:47<13:46, 18.36s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 155] Train Loss: 0.9830\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 156/200 [48:06<13:30, 18.42s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 156] Train Loss: 0.9988\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 157/200 [48:24<13:06, 18.30s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 157] Train Loss: 0.9894\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 158/200 [48:43<12:54, 18.44s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 158] Train Loss: 1.0053\n","output_type":"stream"},{"name":"stderr","text":" 80%|███████▉  | 159/200 [49:01<12:32, 18.36s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 159] Train Loss: 0.9812\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 160/200 [49:20<12:18, 18.46s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 160] Train Loss: 0.9994\n","output_type":"stream"},{"name":"stderr","text":" 80%|████████  | 161/200 [49:38<11:57, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 161] Train Loss: 0.9980\n","output_type":"stream"},{"name":"stderr","text":" 81%|████████  | 162/200 [49:57<11:45, 18.57s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 162] Train Loss: 0.9860\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 163/200 [50:16<11:27, 18.59s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 163] Train Loss: 0.9939\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▏ | 164/200 [50:34<11:05, 18.50s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 164] Train Loss: 0.9817\n","output_type":"stream"},{"name":"stderr","text":" 82%|████████▎ | 165/200 [50:53<10:50, 18.57s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 165] Train Loss: 0.9814\n","output_type":"stream"},{"name":"stderr","text":" 83%|████████▎ | 166/200 [51:11<10:25, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 166] Train Loss: 0.9957\n⚠️ LR reduced at epoch 166 → 1.1e-05\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▎ | 167/200 [51:30<10:14, 18.63s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 167] Train Loss: 0.9918\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 168/200 [51:48<09:56, 18.65s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 168] Train Loss: 0.9825\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 169/200 [52:07<09:33, 18.51s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 169] Train Loss: 0.9769\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▌ | 170/200 [52:25<09:17, 18.57s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 170] Train Loss: 0.9868\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 171/200 [52:44<08:56, 18.49s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 171] Train Loss: 0.9859\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▌ | 172/200 [53:03<08:41, 18.63s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 172] Train Loss: 0.9801\n","output_type":"stream"},{"name":"stderr","text":" 86%|████████▋ | 173/200 [53:21<08:20, 18.52s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 173] Train Loss: 0.9880\n","output_type":"stream"},{"name":"stderr","text":" 87%|████████▋ | 174/200 [53:40<08:03, 18.59s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 174] Train Loss: 0.9760\n","output_type":"stream"},{"name":"stderr","text":" 88%|████████▊ | 175/200 [53:59<07:50, 18.84s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 175] Train Loss: 0.9775\n","output_type":"stream"},{"name":"stderr","text":" 88%|████████▊ | 176/200 [54:17<07:27, 18.63s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 176] Train Loss: 0.9786\n","output_type":"stream"},{"name":"stderr","text":" 88%|████████▊ | 177/200 [54:36<07:08, 18.65s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 177] Train Loss: 0.9696\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 178/200 [54:54<06:47, 18.51s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 178] Train Loss: 0.9815\n","output_type":"stream"},{"name":"stderr","text":" 90%|████████▉ | 179/200 [55:13<06:30, 18.58s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 179] Train Loss: 1.0224\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 180/200 [55:32<06:14, 18.73s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 180] Train Loss: 0.9836\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 181/200 [55:51<05:55, 18.70s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 181] Train Loss: 0.9831\n","output_type":"stream"},{"name":"stderr","text":" 91%|█████████ | 182/200 [56:10<05:39, 18.85s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 182] Train Loss: 0.9817\n⚠️ LR reduced at epoch 182 → 8.6e-06\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 183/200 [56:28<05:17, 18.69s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 183] Train Loss: 0.9913\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▏| 184/200 [56:47<04:58, 18.65s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 184] Train Loss: 0.9780\n","output_type":"stream"},{"name":"stderr","text":" 92%|█████████▎| 185/200 [57:05<04:38, 18.54s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 185] Train Loss: 0.9803\n","output_type":"stream"},{"name":"stderr","text":" 93%|█████████▎| 186/200 [57:24<04:20, 18.59s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 186] Train Loss: 0.9836\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▎| 187/200 [57:42<04:01, 18.56s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 187] Train Loss: 0.9768\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▍| 188/200 [58:00<03:40, 18.36s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 188] Train Loss: 0.9792\n","output_type":"stream"},{"name":"stderr","text":" 94%|█████████▍| 189/200 [58:18<03:21, 18.34s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 189] Train Loss: 0.9770\n","output_type":"stream"},{"name":"stderr","text":" 95%|█████████▌| 190/200 [58:36<03:02, 18.28s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 190] Train Loss: 0.9713\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 191/200 [58:55<02:45, 18.43s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 191] Train Loss: 0.9699\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 192/200 [59:14<02:27, 18.42s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 192] Train Loss: 0.9810\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▋| 193/200 [59:32<02:09, 18.53s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 193] Train Loss: 0.9741\n","output_type":"stream"},{"name":"stderr","text":" 97%|█████████▋| 194/200 [59:51<01:50, 18.49s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 194] Train Loss: 0.9742\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 195/200 [1:00:09<01:31, 18.34s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 195] Train Loss: 0.9824\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 196/200 [1:00:27<01:13, 18.46s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 196] Train Loss: 0.9742\n","output_type":"stream"},{"name":"stderr","text":" 98%|█████████▊| 197/200 [1:00:46<00:55, 18.35s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 197] Train Loss: 0.9754\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 198/200 [1:01:04<00:36, 18.46s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 198] Train Loss: 0.9810\n⚠️ LR reduced at epoch 198 → 6.9e-06\n","output_type":"stream"},{"name":"stderr","text":"100%|█████████▉| 199/200 [1:01:23<00:18, 18.54s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 199] Train Loss: 0.9747\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 200/200 [1:01:43<00:00, 18.52s/it]","output_type":"stream"},{"name":"stdout","text":"[Epoch 200] Train Loss: 0.9738\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"torch.save(vf.state_dict(), \"vf_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T06:44:36.294454Z","iopub.execute_input":"2025-07-14T06:44:36.295674Z","iopub.status.idle":"2025-07-14T06:44:36.323220Z","shell.execute_reply.started":"2025-07-14T06:44:36.295553Z","shell.execute_reply":"2025-07-14T06:44:36.322130Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"root_dir_test = \"./test\"\n\ndef standardize_keys(d):\n    return {\n        os.path.join(os.path.basename(os.path.dirname(k)), os.path.basename(k)): v\n        for k, v in d.items()\n    }\n\n# 1. Load embeddings\nembeddings_test = load_images_recursively(root_dir_test)\nprint(f\" Total images loaded: {len(embeddings_test)}\")\n\n# 2. Load cls_map\ncls_map_test = create_cls_map(root_dir_test)\nprint(f\" Created cls_map for {len(cls_map_test)} images\")\n\n# 3.  Standardize keys\nembeddings_test = standardize_keys(embeddings_test)\ncls_map_test = standardize_keys(cls_map_test)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T06:56:47.270117Z","iopub.execute_input":"2025-07-14T06:56:47.270591Z","iopub.status.idle":"2025-07-14T06:57:26.447748Z","shell.execute_reply.started":"2025-07-14T06:56:47.270559Z","shell.execute_reply":"2025-07-14T06:57:26.446759Z"}},"outputs":[{"name":"stdout","text":" Total images loaded: 284\n Created cls_map for 284 images\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"recall_val = compute_recall_at_1_embedded(embeddings_test, cls_map_test, type_map, vf, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T06:57:39.365309Z","iopub.execute_input":"2025-07-14T06:57:39.365608Z","iopub.status.idle":"2025-07-14T06:57:49.345238Z","shell.execute_reply.started":"2025-07-14T06:57:39.365588Z","shell.execute_reply":"2025-07-14T06:57:49.344157Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"recall_val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T06:57:55.308181Z","iopub.execute_input":"2025-07-14T06:57:55.308496Z","iopub.status.idle":"2025-07-14T06:57:55.319062Z","shell.execute_reply.started":"2025-07-14T06:57:55.308472Z","shell.execute_reply":"2025-07-14T06:57:55.317778Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"0.954225352112676"},"metadata":{}}],"execution_count":52}]}